{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Learning and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Laboratory 6: Reinforcement learning\n",
    "\n",
    "In the end of the lab, you should submit all code/answers written in the tasks marked as \"Activity n. XXX\", together with the corresponding outputs and any replies to specific questions posed to the e-mail <adi.tecnico@gmail.com>. Make sure that the subject is of the form [&lt;group n.&gt;] LAB &lt;lab n.&gt;."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. The windy gridworld domain\n",
    "\n",
    "Consider the larger version of the windy gridworld domain depicted in the figure below.\n",
    "\n",
    "<img src=\"windy.png\" width=\"400px\">\n",
    "\n",
    "In it, a boat must navigate a 7 &times; 10 gridworld, to reach the goal cell, marked with _G_. There is a crosswind upward through the middle of the grid, in the direction indicated by the gray arrows. The boat has available the standard four actions -- _Up_, _Down_, _Left_ and _Right_. In the region affected by the wind, however, the resulting next state is shifted upward as a consequence of the crosswind, the strength of which varies from column to column. The strength of the wind is given below each column, and corresponds to the number of cells that the movement is shifted upward. For example, if the boat is one cell to the right of the goal, then the action _Left_ takes you to the cell just above the goal.\n",
    "\n",
    "The agent pays a cost of 1 in every step before reaching the goal. The problem can be described as an MDP $(\\mathcal{X},\\mathcal{A},\\mathbf{P},c,\\gamma)$ as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- MDP problem specification: -\n",
      "\n",
      "States:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 2]\n",
      " [0 3]\n",
      " [0 4]\n",
      " [0 5]\n",
      " [0 6]\n",
      " [0 7]\n",
      " [0 8]\n",
      " [0 9]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [1 3]\n",
      " [1 4]\n",
      " [1 5]\n",
      " [1 6]\n",
      " [1 7]\n",
      " [1 8]\n",
      " [1 9]\n",
      " [2 0]\n",
      " [2 1]\n",
      " [2 2]\n",
      " [2 3]\n",
      " [2 4]\n",
      " [2 5]\n",
      " [2 6]\n",
      " [2 7]\n",
      " [2 8]\n",
      " [2 9]\n",
      " [3 0]\n",
      " [3 1]\n",
      " [3 2]\n",
      " [3 3]\n",
      " [3 4]\n",
      " [3 5]\n",
      " [3 6]\n",
      " [3 7]\n",
      " [3 8]\n",
      " [3 9]\n",
      " [4 0]\n",
      " [4 1]\n",
      " [4 2]\n",
      " [4 3]\n",
      " [4 4]\n",
      " [4 5]\n",
      " [4 6]\n",
      " [4 7]\n",
      " [4 8]\n",
      " [4 9]\n",
      " [5 0]\n",
      " [5 1]\n",
      " [5 2]\n",
      " [5 3]\n",
      " [5 4]\n",
      " [5 5]\n",
      " [5 6]\n",
      " [5 7]\n",
      " [5 8]\n",
      " [5 9]\n",
      " [6 0]\n",
      " [6 1]\n",
      " [6 2]\n",
      " [6 3]\n",
      " [6 4]\n",
      " [6 5]\n",
      " [6 6]\n",
      " [6 7]\n",
      " [6 8]\n",
      " [6 9]]\n",
      "\n",
      "Actions:\n",
      "['U', 'D', 'L', 'R']\n",
      "\n",
      "Transition probabilities:\n",
      "Action U\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Action D\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "Action L\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]]\n",
      "Action R\n",
      "[[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "\n",
      "cost:\n",
      "[[ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]]\n",
      "\n",
      "Start state: [3, 0]\n",
      "\n",
      "Goal state: [3, 7]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(threshold=1000)\n",
    "\n",
    "# Problem specific parameters\n",
    "WIND = (0, 0, 0, 1, 1, 1, 2, 2, 1, 0)\n",
    "nrows = 7\n",
    "ncols = 10\n",
    "init = [3, 0]\n",
    "goal = [3, 7]\n",
    "\n",
    "# States\n",
    "X = [[x, y] for x in range(nrows) for y in range(ncols)]\n",
    "nX = len(X)\n",
    "\n",
    "# Actions\n",
    "A = ['U', 'D', 'L', 'R']\n",
    "nA = len(A)\n",
    "\n",
    "# Transition probabilities\n",
    "P = dict()\n",
    "P['U'] = np.zeros((nX, nX))\n",
    "P['D'] = np.zeros((nX, nX))\n",
    "P['L'] = np.zeros((nX, nX))\n",
    "P['R'] = np.zeros((nX, nX))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    x = X[i]\n",
    "    y = dict()\n",
    "    \n",
    "    y['U'] = [x[0] - WIND[x[1]] - 1, x[1]]\n",
    "    y['D'] = [x[0] - WIND[x[1]] + 1, x[1]]\n",
    "    y['L'] = [x[0] - WIND[x[1]], x[1] - 1]\n",
    "    y['R'] = [x[0] - WIND[x[1]], x[1] + 1]\n",
    "    \n",
    "    for k in y:\n",
    "        y[k][0] = max(min(y[k][0], nrows - 1), 0)\n",
    "        y[k][1] = max(min(y[k][1], ncols - 1), 0)\n",
    "        j = X.index(y[k])\n",
    "        P[k][i, j] = 1\n",
    "\n",
    "c = np.ones((nX, nA))\n",
    "c[X.index(goal), :] = 0\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "#MDP dictionary structure\n",
    "mdp = {\"X\": X, \"A\": A, \"Pa's\": [P['U'], P['D'], P['L'], P['R']], \"C\":c} \n",
    "\n",
    "# -- Pretty print\n",
    "\n",
    "print('\\n- MDP problem specification: -\\n')\n",
    "\n",
    "print('States:')\n",
    "print(np.array(X))\n",
    "\n",
    "print('\\nActions:')\n",
    "print(A)\n",
    "\n",
    "print('\\nTransition probabilities:')\n",
    "for a in A:\n",
    "    print('Action', a)\n",
    "    print(P[a])\n",
    "    \n",
    "print('\\ncost:')\n",
    "print(c)\n",
    "\n",
    "print('\\nStart state:', init)\n",
    "print('\\nGoal state:', goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 1.        \n",
    "\n",
    "Compute the optimal _Q_-function for the MDP defined above using value iteration. As your stopping condition, use an error between iterations smaller than `1e-8`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2cad71ee9c0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# This takes a while to end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mVI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalueIterationQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMDP\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#print (VI)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2cad71ee9c0d>\u001b[0m in \u001b[0;36mvalueIterationQ\u001b[0;34m(MDP, tolerance, gamma)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMDP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMDP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"A\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mQnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperatorH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMDP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQnew\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2cad71ee9c0d>\u001b[0m in \u001b[0;36moperatorH\u001b[0;34m(MDP, gamma, Q, x, a)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0msum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMDP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maux\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMDP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "mdp_aux = {\"X\": ['0', 'A', 'B'], \n",
    "           \"A\": ['a', 'b'], \n",
    "           \"Pa's\": [[[0, 1, 0], [0, 1, 0], [0, 0, 1]], [[0, 0, 1], [0, 1, 0], [0, 0, 1]]], \n",
    "           \"C\": [[1, 0.5], [0, 0], [1, 1]]} \n",
    "\"\"\"\n",
    "\n",
    "########################### Auxiliar ###############################\n",
    "#@brief: \n",
    "#      This function computes the H operator explained in lecture 8.\n",
    "def operatorH(MDP, gamma, Q, x, a):\n",
    "    sum = 0\n",
    "    for dest in range(0, len(MDP['X'])): \n",
    "        min = float('inf')\n",
    "        for aux in range(0, len(MDP['A'])):\n",
    "            if Q[dest][aux] < min:\n",
    "                min = Q[dest][aux]\n",
    "        sum += MDP[\"Pa's\"][a][x][dest]*min\n",
    "    return MDP['C'][x][a] + gamma*sum\n",
    "    \n",
    "#@brief: \n",
    "#      This function computes the valueIteration algorithm to find the Q function.\n",
    "#\n",
    "#@param: - MDP (dictionary with respective fields)\n",
    "#        - tolerance, which is used to stop the algorithm when Jnew and J are close by a small factor\n",
    "#        - gamma, which represents the inflation.\n",
    "#\n",
    "#@return: \n",
    "#        returns the matrix that represents the Q function and the number of iterations required to compute it.\n",
    "def valueIterationQ(MDP, tolerance, gamma):\n",
    "    Q = np.zeros((len(MDP[\"X\"]), len(MDP[\"A\"]))) # initialize Q\n",
    "    err = 1\n",
    "    i=0  \n",
    "    while err > tolerance:\n",
    "        \n",
    "        Qnew = np.zeros((len(MDP[\"X\"]), len(MDP[\"A\"])))\n",
    "        for x in range(0, len(MDP[\"X\"])):\n",
    "            for a in range(0, len(MDP[\"A\"])):\n",
    "                Qnew[x][a] = operatorH(MDP, gamma, Q, x, a)\n",
    "                \n",
    "        err = la.norm(Qnew - Q)\n",
    "        i += 1\n",
    "        Q = Qnew\n",
    "    return (Q, i)\n",
    "\n",
    "# This takes a while to end.\n",
    "VI, iterations = valueIterationQ(MDP=mdp, tolerance=1e-8, gamma=gamma)\n",
    "\n",
    "#print (VI)\n",
    "#print (iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 2.        \n",
    "\n",
    "Write down a Python function that, given a Q-function $Q$ and a state $x$, selects a random action using the $\\epsilon$-greedy policy obtained from $Q$ for state $x$. Your function should receive an optional parameter, corresponding to $\\epsilon$, with default value of 0.1. \n",
    "\n",
    "**Note:** In the case of two actions with the same value, your $\\epsilon$-greedy policy should randomize between the two.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def eGreedyHeuristic(MDP, Q, x, e=0.1):\n",
    "    greedy = random.randrange(0, 100) < (1-e)*100\n",
    "    \n",
    "    # exploration case\n",
    "    if not greedy:\n",
    "        return random.randrange(0, len(MDP['A']))\n",
    "    \n",
    "    min = float('inf')\n",
    "    action = -1\n",
    "    for a in range(0, len(MDP['A'])):\n",
    "        if Q[x][a] < min:\n",
    "            min = Q[x][a]\n",
    "            action = a\n",
    "\n",
    "    # we now want to count how many actions have the min value and select randomly between them if count > 1\n",
    "    # aux is a list that stores the indexes of the actions with the value equal to the min\n",
    "    aux = []\n",
    "    for a in range(0, len(MDP['A'])):\n",
    "        if Q[x][a] == min:\n",
    "            aux.append(a)\n",
    "            \n",
    "    if len(aux) > 1:\n",
    "        return aux[random.randrange(0, len(aux))]\n",
    "   \n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. Model-based learning\n",
    "\n",
    "You will now run the model-based learning algorithm discussed in class, and evaluate its learning performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 3.        \n",
    "\n",
    "Run the model-based reinforcement learning algorithm discussed in class to compute $Q^*$ for 100,000 iterations. Initialize each transition probability matrix as the identity and the cost function as all-zeros. Use an $\\epsilon$-greedy policy with $\\epsilon=0.1$ (use the function from Activity 2). Note that, at each step,\n",
    "\n",
    "* You will need to select an action according to the $\\epsilon$-greedy policy;\n",
    "* The state and action, you will then compute the cost and generate the next state; \n",
    "* With this transition information (state, action, cost, next-state), you can now perform an update. \n",
    "* When updating the components $(x,a)$ of the model, use the step-size\n",
    "\n",
    "$$\\alpha_t=\\frac{1}{N_t(x,a)+1},$$\n",
    "\n",
    "where $N_t(x,a)$ is the number of visits to the pair $(x,a)$ up to time step $t$.\n",
    "\n",
    "In order to ensure that your algorithm visits every state and action a sufficient number of times, after the boat reaches the goal cell, make one further step, the corresponding update, and then reset the position of the boat to a random state in the environment.\n",
    "\n",
    "Plot the norm $\\|Q^*-Q^{(k)}\\|$ every 500 iterations of your method, where $Q^*$ is the optimal _Q_~function computed in Activity 1.\n",
    "\n",
    "**Note:** The simulation may take a bit. Don't despair.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mb_C = np.zeros((nX, nA))\n",
    "\n",
    "mb_P = dict()\n",
    "mb_P['U'] = np.identity(nX)\n",
    "mb_P['D'] = np.identity(nX)\n",
    "mb_P['L'] = np.identity(nX)\n",
    "mb_P['R'] = np.identity(nX)\n",
    "\n",
    "# the MDP in his initial state is defined as the following dictionary:\n",
    "mb_mdp = {\"X\": X, \"A\": A, \"Pa's\": [mb_P['U'], mb_P['D'], mb_P['L'], mb_P['R']], \"C\":mb_C}\n",
    "\n",
    "x = X.index(init)\n",
    "Q = np.zeros((len(mdp[\"X\"]), len(mdp[\"A\"]))) # initialize Q\n",
    "transition_states = np.zeros((len(mdp['X']), len(mdp['A']), len(mdp['X'])))\n",
    "\n",
    "\n",
    "\"\"\" EST√Å MAL FEITO A PARTIR DAQUI \"\"\"\n",
    "for i in range(0, 5):\n",
    "    action = eGreedyHeuristic(mb_mdp, Q, x)\n",
    "    \n",
    "    # compute the destiny state based on the underlying mdp.\n",
    "    for j in range(0, len(mdp['X'])):\n",
    "        if mdp[\"Pa's\"][action][x][j] == 1:\n",
    "            dest_state = j\n",
    "            break\n",
    "            \n",
    "    # update our matrix that stores the info about the number of actions and destiny for each state\n",
    "    transition_states[x][action][dest_state] += 1\n",
    "    \n",
    "    N_x_a_y = transition_states[x][action][dest_state]\n",
    "    N_x_a =0\n",
    "    for state_count in transition_states[x][action]:\n",
    "        N_x_a += state_count\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. Temporal-difference learning\n",
    "\n",
    "You will now run both Q-learning and SARSA, and compare their learning performance with that of the model-based method just studied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 4.        \n",
    "\n",
    "Repeat Activity 3 but using the _Q_-learning algorithm with a learning rate $\\alpha=0.3$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 87.92471251  87.94130667  87.93643233  87.91584576]\n",
      " [ 87.93148956  87.94963117  87.94346075  87.98588233]\n",
      " [ 88.04695091  88.02215521  88.02435066  88.06897395]\n",
      " [ 88.12276367  88.11647588  88.12015096  88.1304842 ]\n",
      " [ 88.22086235  88.22664344  88.21228097  88.21401785]\n",
      " [ 88.29738919  88.27745439  88.2763003   88.26776923]\n",
      " [ 88.25274062  88.22145121  88.26284855  88.17534607]\n",
      " [ 88.17471797  88.17531838  88.29286516  88.05590519]\n",
      " [ 88.05590519  88.05590513  88.17534613  87.93525777]\n",
      " [ 87.93525777  87.81339168  88.05590519  87.93525777]\n",
      " [ 87.92472923  87.90670599  87.90656132  87.90573414]\n",
      " [ 87.90881659  87.91715049  87.90729498  87.9237606 ]\n",
      " [ 87.95765685  87.92714314  87.94494367  87.96919124]\n",
      " [ 88.06108402  87.98346004  87.98177164  88.0736497 ]\n",
      " [ 88.10128874  88.03623673  88.03333513  88.10177195]\n",
      " [ 88.09154817  87.98574703  88.01205999  88.04956468]\n",
      " [ 88.26311275  88.26256005  88.3067137   88.17534614]\n",
      " [ 88.06489697  88.07969667  88.14783234  88.05585118]\n",
      " [ 88.05590519  88.05590519  88.17534614  87.93525777]\n",
      " [ 87.93525777  87.69029463  88.05590519  87.81339168]\n",
      " [ 87.92485431  87.83725738  87.85893571  87.86179067]\n",
      " [ 87.88620902  87.86190046  87.85936053  87.86224932]\n",
      " [ 87.85660678  87.85569885  87.86601425  87.85763291]\n",
      " [ 87.98411786  87.91361686  87.94030286  87.91237903]\n",
      " [ 87.91376737  87.82103479  87.86971329  87.86862199]\n",
      " [ 87.92893841  87.90450674  87.9560376   87.92125294]\n",
      " [ 87.95590135  87.88701597  87.94308156  87.93434464]\n",
      " [ 88.1576212   88.10320302  88.28643879  88.05590519]\n",
      " [ 88.05560672  87.93517698  88.17271016  87.81339168]\n",
      " [ 87.81339168  87.56595417  87.93525777  87.69029463]\n",
      " [ 87.84043754  87.76844171  87.78392596  87.78578524]\n",
      " [ 87.82673099  87.79031363  87.77730483  87.77568791]\n",
      " [ 87.8107755   87.77896872  87.78612648  87.76562428]\n",
      " [ 87.84505714  87.77137447  87.80466509  87.77257728]\n",
      " [ 87.83049336  87.7782081   87.83389862  87.76233309]\n",
      " [ 87.61858999  87.67397462  87.6146881   87.70765943]\n",
      " [ 87.70305415  87.52390637  87.69547776  87.60513219]\n",
      " [ 87.17534614  87.17534614  87.29359268  87.05590519]\n",
      " [ 88.04902979  87.80851214  88.10868204  87.69029463]\n",
      " [ 87.69029463  87.44035775  87.81339168  87.56595417]\n",
      " [ 87.74951828  87.72269363  87.72127589  87.71770291]\n",
      " [ 87.73971733  87.72112787  87.70698076  87.73498348]\n",
      " [ 87.72876747  87.69326409  87.72195289  87.69113495]\n",
      " [ 87.72322448  87.6975298   87.73821836  87.71430037]\n",
      " [ 87.55777802  87.55050657  87.57880304  87.55538178]\n",
      " [ 87.58378881  87.40358217  87.50572698  87.46950849]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ 88.0958851   87.18534614  87.57806238  87.71000594]\n",
      " [ 87.93525777  87.31349268  87.18534614  87.56595417]\n",
      " [ 87.56595417  87.56595417  87.31349268  87.44035775]\n",
      " [ 87.70983075  87.67071408  87.67272684  87.67278259]\n",
      " [ 87.6546651   87.68218018  87.67570718  87.6711144 ]\n",
      " [ 87.62717761  87.60344159  87.64569907  87.62476691]\n",
      " [ 87.61748756  87.54617981  87.56519887  87.57111077]\n",
      " [ 87.51156263  87.42577648  87.44522813  87.43192442]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ 87.26618938  87.24048233  87.31406408  87.39863514]\n",
      " [ 87.62934994  87.4177162   87.31349268  87.42768708]\n",
      " [ 87.44035775  87.4730232   87.44035775  87.56407539]\n",
      " [ 87.6595466   87.66239558  87.64046004  87.65902945]\n",
      " [ 87.6616883   87.6258559   87.63277677  87.61738228]\n",
      " [ 87.55775211  87.57242351  87.55092124  87.55204063]\n",
      " [ 87.56411231  87.48987376  87.4798989   87.46696456]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ 87.29640684  87.31995699  87.29420728  87.29265802]\n",
      " [ 87.4021066   87.39353985  87.38633655  87.38883419]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "GAMMA = 0.95\n",
    "\n",
    "\n",
    "\n",
    "def q_learning(InitialState,iterations):\n",
    "    \n",
    "    state_t = InitialState\n",
    "    action = eGreedyHeuristic(mdp, Q, state_t, e=0.1)\n",
    "    cost = mdp['C'][state_t][action]\n",
    "    state_t_1 = X.index(X[np.where(mdp[\"Pa's\"][action][state_t]==1)[0][0]])\n",
    "    \n",
    "    \n",
    "    for i in range(0,iterations):\n",
    "        #print(state_t,action,cost,state_t_1)\n",
    "        q_update(state_t,action,cost,state_t_1)\n",
    "    \n",
    "        action = eGreedyHeuristic(mdp, Q, state_t_1, e=0.1)\n",
    "        cost = mdp['C'][state_t_1][action]\n",
    "        \n",
    "        previous = state_t_1\n",
    "        state_t_1 = X.index(X[np.where(mdp[\"Pa's\"][action][previous]==1)[0][0]])\n",
    "        state_t=previous\n",
    "    \n",
    "   \n",
    "    \n",
    "\n",
    "def q_update(state,action,cost,nextState):\n",
    "    \n",
    "    ALPHA=0.3\n",
    "    mini = min(Q[nextState,:])\n",
    "    Q[state][action] = Q[state][action] + ALPHA * (cost + gamma * mini - Q[state][action]) \n",
    "    #print (Q[state][action])\n",
    "\n",
    "\n",
    "x = X.index(init)\n",
    "q_learning(x,100000)\n",
    "print (Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 5.\n",
    "\n",
    "Repeat Activity 4 but using the SARSA algorithm.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 68.7709176   68.77722903  68.8169999   68.72944781]\n",
      " [ 68.81669826  68.87652204  68.85417691  68.91234691]\n",
      " [ 69.16146309  69.01738453  69.04328659  69.04985079]\n",
      " [ 69.26193739  69.34058413  69.28751867  69.26716758]\n",
      " [ 69.4420984   69.46702658  69.4520104   69.47634869]\n",
      " [ 69.71646868  69.73672724  69.69331511  69.7488142 ]\n",
      " [ 69.87505863  69.84819271  69.85644372  69.79527998]\n",
      " [ 69.68733806  69.73071817  69.73899695  69.66597432]\n",
      " [ 69.4606283   69.59660869  69.54909086  69.43770912]\n",
      " [ 69.20161781  69.18491214  69.30118576  69.24016725]\n",
      " [ 68.66023819  68.69008501  68.67253578  68.73599403]\n",
      " [ 68.74231892  68.74715033  68.73905818  68.72355187]\n",
      " [ 68.8201481   68.78878249  68.83055591  68.8610076 ]\n",
      " [ 69.13006212  68.97511463  69.01263459  69.05616407]\n",
      " [ 68.9007905   68.92959117  69.05543872  69.37475318]\n",
      " [ 69.19900125  68.961663    68.96930496  69.03179132]\n",
      " [ 69.38867696  69.25576373  69.18684242  69.32777043]\n",
      " [ 69.38983707  69.07663366  69.3342259   69.26909401]\n",
      " [ 69.21437731  69.19383712  69.36900433  69.152535  ]\n",
      " [ 69.19015493  68.923447    69.00812856  68.96688725]\n",
      " [ 68.61206322  68.54217268  68.54095125  68.58466415]\n",
      " [ 68.62239639  68.54321419  68.60200544  68.62467615]\n",
      " [ 68.69095162  68.60509532  68.56391623  68.57100561]\n",
      " [ 68.89633846  68.65043254  68.7219829   68.69744694]\n",
      " [ 68.6523796   68.68680904  68.79852299  68.60263209]\n",
      " [ 68.63861821  68.42574813  68.56572497  68.58069711]\n",
      " [ 68.49445661  68.71934774  68.51848184  68.63376613]\n",
      " [ 68.99699013  69.05969759  68.93069359  68.99261276]\n",
      " [ 68.89048062  68.67088733  68.77013261  68.76893429]\n",
      " [ 68.71692671  68.65911738  68.67472305  68.72850999]\n",
      " [ 68.40755627  68.38362704  68.36377173  68.40281463]\n",
      " [ 68.38139379  68.37340639  68.37016262  68.44117729]\n",
      " [ 68.4430471   68.33006326  68.3752232   68.47544511]\n",
      " [ 68.66384633  68.46822018  68.50887262  68.43284607]\n",
      " [ 68.4011006   68.2038698   68.28956335  68.28981896]\n",
      " [ 68.05351572  67.66394525  67.78751364  67.84458054]\n",
      " [ 67.71493126  67.89189964  68.17505588  68.32418869]\n",
      " [ 68.04332641  67.92420502  68.04310563  67.94199108]\n",
      " [ 68.72689742  68.36751383  68.45425645  68.4854822 ]\n",
      " [ 68.63205548  68.40932825  68.3700828   68.43528999]\n",
      " [ 68.28826067  68.21349012  68.20406777  68.21060316]\n",
      " [ 68.28423274  68.15882456  68.18913209  68.22836943]\n",
      " [ 68.24203912  68.11626202  68.17003289  68.16243584]\n",
      " [ 68.3024652   68.04428617  68.20844011  68.10827356]\n",
      " [ 67.96720034  67.76445082  67.66314708  67.64131782]\n",
      " [ 67.78079834  67.36340307  67.65569336  67.4785986 ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ 68.25497798  67.81078583  67.83501766  67.89328353]\n",
      " [ 67.99415523  68.06553585  68.04177719  68.0192255 ]\n",
      " [ 68.181766    68.13556823  68.180024    68.16820378]\n",
      " [ 68.11363962  68.12100677  68.07521248  68.11094917]\n",
      " [ 68.08776759  68.01532636  68.00077124  68.01344515]\n",
      " [ 67.99569735  67.94782628  67.89327065  67.92466139]\n",
      " [ 68.09386978  67.75423508  67.74206085  67.74911087]\n",
      " [ 67.66726843  67.44893682  67.44144182  67.46628009]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ 67.67971042  67.59103787  67.52183632  67.65554595]\n",
      " [ 67.86133839  67.68855904  67.74113383  67.90783218]\n",
      " [ 67.88910661  67.90541103  67.90628594  67.89964415]\n",
      " [ 68.03593156  67.99372297  68.0287126   68.02327221]\n",
      " [ 67.91252433  67.86338393  67.96816657  67.92466459]\n",
      " [ 67.837165    67.851109    67.7549866   67.80262278]\n",
      " [ 67.70226183  67.64800052  67.75559441  67.69574671]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ 67.59439791  67.57530795  67.58612706  67.69893363]\n",
      " [ 67.78931825  67.77120564  67.72888089  67.75523044]]\n"
     ]
    }
   ],
   "source": [
    "sarsaMatrix = np.zeros((len(mdp[\"X\"]), len(mdp[\"A\"])))\n",
    "\n",
    "def sarsa(InitialState,iterations):\n",
    "    \n",
    "    state_t = InitialState\n",
    "    action_t = eGreedyHeuristic(mdp, sarsaMatrix, state_t, e=0.1)\n",
    "    cost = mdp['C'][state_t][action_t]\n",
    "    state_t_1 = X.index(X[np.where(mdp[\"Pa's\"][action_t][state_t]==1)[0][0]])\n",
    "    action_t_1 = eGreedyHeuristic(mdp, sarsaMatrix, state_t_1, e=0.1)\n",
    "    \n",
    "    \n",
    "    for i in range(0,iterations):\n",
    "        #print(state_t,action,cost,state_t_1)\n",
    "        sarsa_update(state_t,action_t,cost,state_t_1,action_t_1)\n",
    "        action_t = eGreedyHeuristic(mdp, sarsaMatrix, state_t_1, e=0.1)\n",
    "        cost = mdp['C'][state_t_1][action_t]\n",
    "        previous = state_t_1\n",
    "        state_t_1 = X.index(X[np.where(mdp[\"Pa's\"][action_t][previous]==1)[0][0]])\n",
    "        state_t = previous\n",
    "        action_t_1 = eGreedyHeuristic(mdp, sarsaMatrix, state_t_1, e=0.1)\n",
    "\n",
    "def sarsa_update(state,action,cost,nextState,nextAction):\n",
    "    \n",
    "    ALPHA=0.3\n",
    "    sarsaMatrix[state][action] = sarsaMatrix[state][action] + ALPHA * (cost + gamma * sarsaMatrix[nextState][nextAction] - sarsaMatrix[state][action]) \n",
    "    #print (Q[state][action])\n",
    "\n",
    "\n",
    "x = X.index(init)\n",
    "sarsa(x,100000)\n",
    "print (sarsaMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 6.\n",
    "\n",
    "Discuss the differences observed between the performance of the three methods.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "_Add your discussion here._"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
