{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Learning and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Laboratory 4: Partially observable Markov decision problems\n",
    "\n",
    "In the end of the lab, you should submit all code/answers written in the tasks marked as \"Activity n. XXX\", together with the corresponding outputs and any replies to specific questions posed to the e-mail <adi.tecnico@gmail.com>. Make sure that the subject is of the form [&lt;group n.&gt;] LAB &lt;lab n.&gt;."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. Modeling\n",
    "\n",
    "Consider once again the guessing game domain described in the Homework and which you described as a POMDP.\n",
    "\n",
    "Recall that:\n",
    "\n",
    "* The opponent can hold one of two cards in hand: an Ace of Clubs (A&clubs;) and an Ace of Diamonds (A&diams;). The agent must guess which card the opponent is holding. \n",
    "\n",
    "* For every right answer, the agent wins 1EUR, and every wrong answer costs the agent 1EUR. \n",
    "\n",
    "* The agent can also try to _peek_. \n",
    "\n",
    "* When the agent peeks, it sees the right card with a probability of 0.9 and the wrong card with probability 0.1.\n",
    "\n",
    "* The game restarts whenever the agent makes a guess.\n",
    "\n",
    "Consider throughout that $\\gamma=0.9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 1.        \n",
    "\n",
    "Implement your POMDP in Python. In particular,\n",
    "\n",
    "* Create a list with all the states;\n",
    "* Create a list with all the actions;\n",
    "* Create a list with all the observations\n",
    "* For each action, define a `numpy` array with the corresponding transition probabilities;\n",
    "* For each action, define a `numpy` array with the corresponding observation probabilities;\n",
    "* Define a `numpy`array with the cost that you defined in your homework.\n",
    "\n",
    "The order for the states and actions used in the transition probability and cost matrices should match that in the lists of states and actions. \n",
    "\n",
    "**Note**: Don't forget to import `numpy`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import *\n",
    "\n",
    "X = ['AC', 'AD'] # 'AC' = ace of clubs, 'AD' = ace of diamonds\n",
    "\n",
    "A = ['GC', 'GD', 'PEEK'] # Guess Clubs, Guess Diamonds, Peek\n",
    "\n",
    "Z = ['OC','OD','NOTHING'] # Observe Clubs, Observe Diamonds, Nothing\n",
    "     \n",
    "\n",
    "P_GC = np.array([[0.5, 0.5],\n",
    "                 [0.5, 0.5]])\n",
    "     \n",
    "P_GD = np.array([[0.5, 0.5],\n",
    "                 [0.5, 0.5]])\n",
    "\n",
    "P_PEEK = np.array([[1, 0],\n",
    "                   [0, 1]])\n",
    "     \n",
    "O_GC = np.array([[0, 0, 1],\n",
    "                 [0, 0, 1]])\n",
    "\n",
    "O_GD = np.array([[0, 0, 1],\n",
    "                 [0, 0, 1]])\n",
    "     \n",
    "O_PEEK = np.array([[0.9, 0.1, 0],\n",
    "                   [0.1, 0.9, 0]])\n",
    "\n",
    "\n",
    "C = np.array([[0,1,0.1],\n",
    "              [1,0,0.1]])\n",
    "\n",
    "\n",
    "# This dictionary will represent the POMDP in our code.\n",
    "# kEY description: \"X\" = space of states, \n",
    "#                  \"A\" = possible actions,\n",
    "#                  \"Z\" = possible observations,\n",
    "#                  \"Pa's\" = transition matrix for actions in A (MDP[\"A\"] and MDP[\"Pa's\"] must respect the same order)\n",
    "#                  \"Oa's\" = observation matrix for actions in A (MDP[\"A\"] and MDP[\"Oa's\"] must respect the same order) \n",
    "#                  \"C\" = cost function for the MDP\n",
    "\n",
    "pomdp = {\"X\": X, \"A\": A, \"Z\": Z, \"Pa's\": [P_GC, P_GD, P_PEEK], \"Oa's\": [O_GC, O_GD, O_PEEK], \"C\":C}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. Sampling\n",
    "\n",
    "You are now going to sample random trajectories of your POMDP and observe the impact it has on the corresponding belief."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "raw_mimetype": "text/latex"
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 2.\n",
    "\n",
    "Generate a random POMDP trajectory using a uniformly random policy. In particular, from a random initial state $x_0$ generate:\n",
    "\n",
    "1. A sequence of 10,000 states by selecting the actions uniformly at random;\n",
    "2. The corresponding sequence of 10,000 actions;\n",
    "3. The corresponding sequence of 10,000 observations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def generateRandomTrajectory(POMDP, policy, s0, steps=10):  #only 100 for debugging\n",
    "    states = np.array([s0])\n",
    "    actions = np.array([])\n",
    "    observations = np.array([])\n",
    "    \n",
    "    for i in range(0, steps):\n",
    "        random_action = np.random.choice(len(POMDP['A']), size=1, p=policy[states[i]])\n",
    "        possible_state = np.random.choice(len(POMDP['X']), size=1, p=POMDP[\"Pa's\"][random_action][states[i]])\n",
    "        possible_observation = np.random.choice(len(POMDP['Z']), size=1, p=POMDP[\"Oa's\"][random_action][states[i]])\n",
    "        \n",
    "        states = np.append(states, possible_state)\n",
    "        actions = np.append(actions, random_action)\n",
    "        observations = np.append(observations, possible_observation)\n",
    "\n",
    "    return [states, actions, observations]\n",
    "\n",
    "initial_state = 0\n",
    "\n",
    "random_policy =np.array([[1/3, 1/3, 1/3],\n",
    "                         [1/3, 1/3, 1/3]]);\n",
    "\n",
    "[states, actions, observations] = generateRandomTrajectory(pomdp, random_policy, initial_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 3.\n",
    "\n",
    "For the POMDP trajectory generated in Activity 2, compute the corresponding sequence of beliefs, assuming that the initial belief is $\\mathbf{b}_0=[0.5, 0.5]$. Report the resulting beliefs, ignoring duplicate beliefs or beliefs whose distance is smaller than $10^{-4}$.\n",
    "\n",
    "**Note 1:** You may want to define a function `belief_update` that receives a belief, an action and an observation and returns the updated belief.\n",
    "\n",
    "**Note 2:** To compute the distance between vectors, you may find useful `numpy`'s function `linalg.norm`.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ nan,  nan])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def beliefUpdate(POMDP, belief, action, observation):\n",
    "    a1_hat = np.dot(belief, POMDP[\"Pa's\"][int(action)])\n",
    "    diag = np.diagflat(POMDP[\"Oa's\"][int(action)][:,int(observation)])\n",
    "    a1 = np.dot(a1_hat, diag)\n",
    "    norm_a1 = a1 / np.sum(a1)\n",
    "    \n",
    "    return norm_a1\n",
    "\n",
    "beliefUpdate(pomdp, np.array([0.5,0.5]), 2, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. Solution methods\n",
    "\n",
    "In this section you are going to compare different non-exact solution methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 4\n",
    "\n",
    "Compute the solution for the underlying MDP and report the corresponding optimal policy and optimal cost-to-go. \n",
    "\n",
    "** Note:** You may reuse code from previous labs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# underlying MDP\n",
    "mdp = {\"X\": pomdp['X'], \"A\": pomdp['A'], \"Pa's\": pomdp[\"Pa's\"], \"C\": pomdp['C']} \n",
    "\n",
    "#@brief: \n",
    "#      This function computes the policy iteration algorithm to find the optimal policy (π*).\n",
    "#\n",
    "#@param: - MDP dictionary\n",
    "#        \n",
    "#@return: \n",
    "#        returns a tuple that contains the optimal policy (π*) and the number of iterations required to compute it.\n",
    "def policyIteration(MDP, gamma):\n",
    "    pi = np.ones((len(MDP['X']), len(MDP['A']))) /2\n",
    "    quit = False\n",
    "    i = 0\n",
    "    \n",
    "    while not quit:\n",
    "        \n",
    "        #initialize cπ and pπ with first line of pi multiplyed by the costs for the first action\n",
    "        cpi = np.diag(pi[:,0]).dot(MDP[\"C\"][:,[0]])\n",
    "        ppi = np.diag(pi[:, 0]).dot(MDP[\"Pa's\"][0])\n",
    "        \n",
    "        # loop over the rest of the actions \n",
    "        for a in range(1, len(MDP[\"A\"])):\n",
    "            cpi += np.diag(pi[:,a]).dot(MDP[\"C\"][:,[a]])\n",
    "            ppi += np.diag(pi[:, a]).dot(MDP[\"Pa's\"][a])\n",
    "\n",
    "        J = np.linalg.inv(np.eye(len(MDP[\"X\"])) - gamma * ppi).dot(cpi)\n",
    "        \n",
    "        Qa = [None] * len(MDP[\"A\"])\n",
    "        \n",
    "        # loop over every action\n",
    "        for a in range(0, len(MDP[\"A\"])):\n",
    "            Qa[a] = MDP[\"C\"][:,[a]] + gamma * MDP[\"Pa's\"][a].dot(J)\n",
    "        \n",
    "        pinew = np.zeros((len(MDP['X']), len(MDP['A'])))\n",
    "        \n",
    "        # loop over every action\n",
    "        for a in range(0, len(MDP[\"A\"])):\n",
    "            pinew[:, a, None] = np.isclose(Qa[a], np.min(Qa, axis=0), atol=1e-8, rtol=1e-8).astype(int)\n",
    "\n",
    "        pinew = pinew / np.sum(pinew, axis=1, keepdims=True)\n",
    "        quit = (pi == pinew).all()\n",
    "        pi = pinew\n",
    "        i +=1\n",
    "    return (pi, i)\n",
    "\n",
    "#@brief: \n",
    "#      This function computes the valueIteration algorithm to find the optimal cost to go J*.\n",
    "#\n",
    "#@param: - MDP (as explained in activity 1)\n",
    "#        - tolerance, which is used to stop the algorithm when Jnew and J are close by a small factor\n",
    "#        - gamma, which represents the inflation.\n",
    "#\n",
    "#@return: \n",
    "#        returns a tuple that contains J* and the number of iterations required to compute J*\n",
    "def valueIteration(MDP, tolerance, gamma):\n",
    "    \n",
    "    #@brief:\n",
    "    #      computes Jnew.\n",
    "    #\n",
    "    #@param: - Qa, is a vector with the Q for every action.\n",
    "    #\n",
    "    #@return: \n",
    "    #       returns a column vector with the minimun value in every line of Qa.\n",
    "    def computeJnew(Qa):\n",
    "        Jnew = np.zeros((len(Qa[0]),1))\n",
    "        for i in range(0, len(Qa[0])):\n",
    "            min = Qa[0][i]\n",
    "            for j in range(1, len(Qa)):\n",
    "                if Qa[j][i] < min:\n",
    "                    min = Qa[j][i]\n",
    "            Jnew[i][0] = min\n",
    "        return Jnew\n",
    "    \n",
    "    J = np.zeros((len(MDP[\"X\"]), 1)) # initialize J\n",
    "    err = 1\n",
    "    i=0\n",
    "    while err > tolerance:\n",
    "        \n",
    "        Qa = [None]*len(MDP[\"A\"]) #initialize empty list for Q values for actions a in A\n",
    "        \n",
    "        # loop over actions and compute Qa\n",
    "        for a in range(0, len(MDP[\"A\"])):\n",
    "            Qa[a] = MDP[\"C\"][:,[a]] + gamma * MDP[\"Pa's\"][a].dot(J)\n",
    "        \n",
    "        Jnew = computeJnew(Qa)\n",
    "        err = np.linalg.norm(Jnew - J)\n",
    "        i += 1\n",
    "        J = Jnew    \n",
    "    return (J, i)\n",
    "\n",
    "optimal_policy, iterations = policyIteration(MDP=mdp, gamma=0.9)\n",
    "J_optimal, iterations = valueIteration(MDP=mdp, tolerance=1e-8, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 5\n",
    "\n",
    "For each of the beliefs computed in Activity 3, compute the action prescribed by:\n",
    "\n",
    "* The MLS heuristic;\n",
    "* The AV heuristic;\n",
    "* The Q-MDP heuristic.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None]\n",
      "0.3\n",
      "0.7\n",
      "0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "################################### Functions From Lab 3 ########################################################\n",
    "#@brief:\n",
    "#   Computes the costs for that policy (Cπ).\n",
    "#\n",
    "#@param: - costs, matrix that represents the costs for every state\n",
    "#        - policy, represents the policy we want to know the costs.\n",
    "#\n",
    "#@return: \n",
    "#        returns the costs for the policy (Cπ).\n",
    "def calculate_Cpi(costs, policy):\n",
    "    cpi = np.zeros(len(policy))\n",
    "   \n",
    "    # loop over states\n",
    "    for i in range(0, len(policy)):        \n",
    "        # loop over actions to compute sum(pi(a|x) * c(a,x))\n",
    "\n",
    "        sum = 0\n",
    "        for j in range(0, len(costs[0])):\n",
    "            sum += policy[i][j]*costs[i][j]\n",
    "\n",
    "        cpi[i] = sum\n",
    "    return cpi\n",
    "\n",
    "#@brief:\n",
    "#   Computes the transiction matrix for a specific policy (Pπ).\n",
    "#\n",
    "#@param: - MDP (as explained in activity 1).\n",
    "#        - policy, that represents the policy we want to know the transiction matrix.\n",
    "#\n",
    "#@return: \n",
    "#        returns the transiction matrix for the policy (Pπ).\n",
    "def transition_Probabilities_For_Policy(MDP, policy):\n",
    "    Ppi = np.zeros((len(MDP[\"X\"]),len(MDP[\"X\"]))) # creates a matrix NxN where N is the size of X\n",
    "    \n",
    "    for i in range(0, len(MDP[\"X\"])):\n",
    "        for j in range(0, len(MDP[\"X\"])):\n",
    "            \n",
    "            # for every entry we need to loop over possible actions to compute ∑ π(a|x)*P(y|x,a)\n",
    "            sum = 0\n",
    "            for k in range(0,len(MDP[\"A\"])):\n",
    "                sum += policy[i][k] * MDP[\"Pa's\"][k][i][j]\n",
    "            \n",
    "            Ppi[i][j] = sum\n",
    "    return Ppi\n",
    "\n",
    "########################################## Heuristics ##########################################################\n",
    "\n",
    "#@brief:\n",
    "#      computes the next action acording to a policy and a belief using the MLS Heuristic studied in \n",
    "#      theoretical lessons.\n",
    "#\n",
    "#@param: - belief, vector with the beliefs for every state.\n",
    "#        - policy, matrix that represents the policy we want to follow.\n",
    "#@return: \n",
    "#       returns an integer wich is the index of the action in POMDP dictionary.\n",
    "def mlsHeuristic(belief,policy):\n",
    "    index = np.argmax(belief)\n",
    "    action = np.argmax(policy[index])\n",
    "    return action\n",
    "\n",
    "\n",
    "#@brief:\n",
    "#      computes the next action acording to a policy and a belief using the AV Heuristic studied in \n",
    "#      theoretical lessons.\n",
    "#\n",
    "#@param: - belief, vector with the beliefs for every state.\n",
    "#        - policy, matrix that represents the policy we want to follow.\n",
    "#@return: \n",
    "#       returns an integer wich is the index of the action in POMDP dictionary.\n",
    "def avHeuristic(belief, policy):\n",
    "    \n",
    "    max = -1\n",
    "    max_action = None\n",
    "    for j in range(0, len(policy[0])): # loop over actions\n",
    "        sum = 0\n",
    "        for i in range(0, len(policy)): # loop over states\n",
    "            sum += policy[i][j]*belief[i] # b(x)*I(a = π mdp(x))\n",
    "            \n",
    "        if (sum > max):\n",
    "            max = sum\n",
    "            max_action = j # stores the action.\n",
    "            \n",
    "    return max_action\n",
    "\n",
    "# test: avHeuristic([0.2, 0.2, 0.6], np.array([[0.2, 0.8, 0], [0, 1, 0], [0.5, 0, 0.5]]))\n",
    "\n",
    "#@brief:\n",
    "#      computes the next action acording to a policy and a belief using the QMDP Heuristic studied in \n",
    "#      theoretical lessons.\n",
    "#\n",
    "#@param: - belief, vector with the beliefs for every state.\n",
    "#        - policy, matrix that represents the policy we want to follow.\n",
    "#        - POMDP, dictionary with the structure that reprensents the POMDP. This is required for calculating Q's.\n",
    "#@return: \n",
    "#       returns an integer wich is the index of the action in POMDP dictionary.\n",
    "def QMDPHeuristic(belief, policy, POMDP):\n",
    "    gamma = 0.9\n",
    "    Cpi = calculate_Cpi(POMDP[\"C\"], policy)\n",
    "    Ppi = transition_Probabilities_For_Policy(POMDP, policy)\n",
    "    Jpi = np.dot(np.linalg.inv((np.identity(2)-(gamma*Ppi))), Cpi)\n",
    "\n",
    "    Qa = [None]*len(POMDP[\"A\"]) #initialize empty list for Q values for actions a in A\n",
    "    print (Qa)\n",
    "    \n",
    "    # loop over actions and compute Qa\n",
    "    for a in range(0, len(POMDP[\"A\"])):\n",
    "        Qa[a] = POMDP[\"C\"][:,a] + gamma * POMDP[\"Pa's\"][a].dot(Jpi)\n",
    "            \n",
    "    # loop over action and for every action a calculate the inner product between Qa and the belief in order\n",
    "    # to find the action that minimizes Qa.belief.\n",
    "    min = sys.maxsize\n",
    "    action = None\n",
    "    for a in range(0, len(POMDP[\"A\"])):\n",
    "        if np.dot(belief, Qa[a]) < min:\n",
    "            action = a\n",
    "            min = np.dot(belief, Qa[a])\n",
    "            \n",
    "    return action\n",
    "\n",
    "QMDPHeuristic(np.array([0.7, 0.3]), optimal_policy, pomdp)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 6\n",
    "\n",
    "Suppose that the optimal cost-to-go function for the POMDP can be represented using the $\\alpha$-vectors\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{bmatrix}\n",
    "2.795\\\\\n",
    "3.795\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "3.795\\\\\n",
    "2.795\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "3.105\\\\\n",
    "3.105\n",
    "\\end{bmatrix}\\right\\}$$\n",
    "\n",
    "corresponding to the actions 'Guess clubs', 'Guess diamonds' and 'Peek', respectively. Represent the optimal cost-to-go function and compare the optimal policy with the MDP heuristics from Activity 5 in the beliefs computed in Activity 3.\n",
    "\n",
    "** Note: ** Don't forget to import `matplotlib`, and use the magic `%matplotlib notebook`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
